{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction\n",
    "\n",
    "In this notebook, we are going to predict the next word that the writer is going to write. This will help us evaluate that how much the neural network has understood about dependencies between different letters that combine to form a word. We can also get an idea of how much the model has understood about the order of different types of word in a sentence.\n",
    "\n",
    "Code segments [1] to [5] are same as that in 'train.ipynb' notebook and their detailed explanation can be found over their itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel(VOCABULARY):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape = (SEQ_LENGTH, 1), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(VOCABULARY, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('wonderland.txt', encoding = 'utf8')\n",
    "raw_text = file.read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n",
      "['\\n', ' ', '!', '$', '%', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)\n",
    "bad_chars = ['#', '*', '@', '_', '\\ufeff']\n",
    "for i in range(len(bad_chars)):\n",
    "    raw_text = raw_text.replace(bad_chars[i],\"\")\n",
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)\n",
    "VOCABULARY = len(chars)\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been defined and we have preprocessed our input file and redefinded our vocabulary, as in train.ipynb file we are ready to proceed. The best model with least loss as we obtained in the last epoch of training is loaded and the model is build and recompiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'saved_models/weights-improvement-49-1.3420.hdf5'\n",
    "model = buildmodel(VOCABULARY)\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original model has been defined in a manner to take in 100 character inputs. So when the user initially starts typing the words, the total length of input string will be less than 100 characters. To solve this issue, the input has been padded with series of spaces in the beginning in ordert to make the total length of 100 characters. As the total length exceeds 100 characters, only last 100 characters as the LSTM nodes take care of remembering the context of the document from before.\n",
    "\n",
    "Succeeding characters are predicted by the model until a space or full stop is encountered. The predicted characters are joined to form the next word, predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e42f0f546747b2ac49560bd9b8183c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " use | of | the |"
     ]
    }
   ],
   "source": [
    "original_text = []\n",
    "predicted_text = []\n",
    "\n",
    "text = widgets.Text()\n",
    "display(text)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    global predicted_text\n",
    "    global original_text\n",
    "    \n",
    "    inp = list(text.value)\n",
    "    \n",
    "    last_word = inp[len(original_text):]\n",
    "    inp = inp[:len(original_text)]    \n",
    "    original_text = text.value    \n",
    "    last_word.append(' ')\n",
    "    \n",
    "    inp_text = [char_to_int[c] for c in inp]\n",
    "    last_word = [char_to_int[c] for c in last_word]\n",
    "    \n",
    "    if len(inp_text) > 100:\n",
    "        inp_text = inp_text[len(inp_text)-100: ]\n",
    "    if len(inp_text) < 100:\n",
    "        pad = []\n",
    "        space = char_to_int[' ']\n",
    "        pad = [space for i in range(100-len(inp_text))]\n",
    "        inp_text = pad + inp_text\n",
    "    \n",
    "    while len(last_word)>0:\n",
    "        X = np.reshape(inp_text, (1, SEQ_LENGTH, 1))\n",
    "        next_char = model.predict(X/float(VOCABULARY))\n",
    "        inp_text.append(last_word[0])\n",
    "        inp_text = inp_text[1:]\n",
    "        last_word.pop(0)\n",
    "    \n",
    "    next_word = []\n",
    "    next_char = ':'\n",
    "    while next_char != ' ':\n",
    "        X = np.reshape(inp_text, (1, SEQ_LENGTH, 1))\n",
    "        next_char = model.predict(X/float(VOCABULARY))\n",
    "        index = np.argmax(next_char)        \n",
    "        next_word.append(int_to_char[index])\n",
    "        inp_text.append(index)\n",
    "        inp_text = inp_text[1:]\n",
    "        next_char = int_to_char[index]\n",
    "    \n",
    "    predicted_text = predicted_text + [''.join(next_word)]\n",
    "    print(\" \" + ''.join(next_word), end='|')\n",
    "    \n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
